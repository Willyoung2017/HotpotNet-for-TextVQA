{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TextVQA_TextAnalysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Willyoung2017/Team_Hotpot_11777/blob/main/Data-Analysis/TextVQA_TextAnalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "u9wLK14Qg_Oz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "KKU2-Y3e57c6"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import csv\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "# import seaborn as sb\n",
        "import spacy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load English language pretrained model from spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "zCzXo0vgXB18"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load json data\n",
        "# Note: may need to edit the file paths\n",
        "with open('/content/drive/MyDrive/11777/TextVQA_0.5.1_val.json') as val, \\\n",
        "    open('/content/drive/MyDrive/11777/TextVQA_0.5.1_train.json') as train, \\\n",
        "    open('/content/drive/MyDrive/11777/TextVQA_0.5.1_test.json') as test:\n",
        "    val_list = json.load(val)\n",
        "    train_list = json.load(train)\n",
        "    test_list = json.load(test)"
      ],
      "metadata": {
        "id": "4hF3h0jbXQaA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert json data to pandas dfs\n",
        "val_data = pd.json_normalize(val_list, record_path='data')\n",
        "train_data = pd.json_normalize(train_list, record_path='data')\n",
        "test_data = pd.json_normalize(test_list, record_path='data')"
      ],
      "metadata": {
        "id": "HtmMj0q3XlD_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# val_data.info()"
      ],
      "metadata": {
        "id": "Dp9oijaZaqWt"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For this analysis, select text-related columns from the _data dfs to create _text dfs\n",
        "val_text = val_data[['question_id', 'question', 'question_tokens', 'answers', 'set_name']]\n",
        "train_text = train_data[['question_id', 'question', 'question_tokens', 'answers', 'set_name']]\n",
        "test_text = test_data[['question_id', 'question', 'question_tokens', 'set_name']]"
      ],
      "metadata": {
        "id": "vTSfB6q0be-w"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add a column on question length, and another column that caps question length at 20\n",
        "for text_df in [val_text, train_text, test_text]:\n",
        "    text_df['question_len'] = text_df.question_tokens.apply(len)\n",
        "    text_df['capped_question_len'] = text_df.apply(lambda x: x['question_len'] if x['question_len'] <= 20 else 20, axis=1)"
      ],
      "metadata": {
        "id": "Ei_cz-QXkOZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add a column on the number of answers for each question\n",
        "val_text['answer_num'] = val_text.answers.apply(len)\n",
        "train_text['answer_num'] = train_text.answers.apply(len)"
      ],
      "metadata": {
        "id": "bClNBaNKk5Sh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This cell and the next confirm that each question has 10 answers\n",
        "val_text.describe()"
      ],
      "metadata": {
        "id": "4ZlIjMFHlu14"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_text.describe()"
      ],
      "metadata": {
        "id": "hYQbB_CPlvje"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the question column to a list of docs\n",
        "val_questions = list(nlp.pipe(val_data.question))\n",
        "train_questions = list(nlp.pipe(train_data.question))\n",
        "test_questions = list(nlp.pipe(test_data.question))"
      ],
      "metadata": {
        "id": "sVmMA-8orLhN"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# type(val_questions[0])"
      ],
      "metadata": {
        "id": "ApyS-1Q8xfxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the answers column to a df with 10 columns, containing answers only\n",
        "# The expanded_ dfs have only one column that contains all answers\n",
        "val_10_answers = val_text[\"answers\"].apply(pd.Series)\n",
        "expanded_val_answers = val_10_answers.melt()\n",
        "train_10_answers = train_text[\"answers\"].apply(pd.Series)\n",
        "expanded_train_answers = train_10_answers.melt()"
      ],
      "metadata": {
        "id": "ueddTi1CpwCs"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This function converts an entire df to a list of docs \n",
        "def df_to_docs(df):\n",
        "    docs = []\n",
        "    for name, series in df.items():\n",
        "        docs = docs + list(nlp.pipe(series))\n",
        "    return docs"
      ],
      "metadata": {
        "id": "Nq3YuqAtwTC-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert each answers df to a list of docs\n",
        "val_answers = df_to_docs(val_10_answers)\n",
        "train_answers = df_to_docs(train_10_answers)"
      ],
      "metadata": {
        "id": "zi5bJIhcyNzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(len(val_answers))\n",
        "# print(val_10_answers.shape)"
      ],
      "metadata": {
        "id": "DnYOZmuj2KDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This function extracts the token data of interest from a doc\n",
        "def extract_tokens_plus_meta(doc):\n",
        "    return [\n",
        "        (i.text, i.i, i.pos_) for i in doc\n",
        "    ]"
      ],
      "metadata": {
        "id": "_HLFL0anrbFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This function uses a list of docs to tabulate token data info of interest to a df\n",
        "def tab_token_data(docs):\n",
        "    \"\"\"Extract tokens and metadata from list of spaCy docs.\"\"\"\n",
        "    \n",
        "    cols = [\n",
        "        \"doc_id\", \"token\", \"token_order\", \"pos\"\n",
        "    ]\n",
        "    \n",
        "    meta_df = []\n",
        "    for ix, doc in enumerate(docs):\n",
        "        meta = extract_tokens_plus_meta(doc)\n",
        "        meta = pd.DataFrame(meta)\n",
        "        meta.columns = cols[1:]\n",
        "        meta = meta.assign(doc_id = ix).loc[:, cols]\n",
        "        meta_df.append(meta)\n",
        "        \n",
        "    return pd.concat(meta_df)  "
      ],
      "metadata": {
        "id": "4T3tnRTHrZwb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dfs that contain question/ answer token data of interest\n",
        "val_q_token_data = tab_token_data(val_questions)\n",
        "train_q_token_data = tab_token_data(train_questions)\n",
        "test_q_token_data = tab_token_data(test_questions)\n",
        "val_a_token_data = tab_token_data(val_answers)\n",
        "train_a_token_data = tab_token_data(train_answers)"
      ],
      "metadata": {
        "id": "vR4p0ItsxH8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# val_q_token_data.query(\"pos != 'PUNCT'\").pos.value_counts()"
      ],
      "metadata": {
        "id": "0vTk8OXvxHs_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For each dataset, analyze questions and create graphs and metrics files\n",
        "# Specifically, question length, the most frequent questions, question starting word, and the most frequent word classes in questions\n",
        "\n",
        "def analyze_questions(text_df, token_data, prefix, path):\n",
        "    \n",
        "    avg_question_len = text_df['question_len'].mean()\n",
        "    median_question_len = text_df['question_len'].median()\n",
        "    std_dev_question_len = text_df['question_len'].std()\n",
        "    with open(path + prefix + 'question_metrics.txt', 'w') as metrics_file:\n",
        "        tsv_writer = csv.writer(metrics_file, delimiter='\\t')\n",
        "        tsv_writer.writerow(['avg question length: ' + \"%.2f\" % avg_question_len])\n",
        "        tsv_writer.writerow(['median question length: ' + \"%.2f\" % median_question_len])\n",
        "        tsv_writer.writerow(['std dev of question length: ' + \"%.2f\" % std_dev_question_len])\n",
        "    \n",
        "    plt.clf()\n",
        "    text_df.capped_question_len.hist(figsize=(14, 7), range=(0, 20), color=\"red\", alpha=.4, bins=20)\n",
        "    plt.savefig(path + prefix + 'question_length.png')\n",
        "\n",
        "    plt.clf()\n",
        "    text_df.question.value_counts().head(10).plot(kind=\"barh\", figsize=(30, 14), color='green', alpha=.7)\n",
        "    plt.yticks(fontsize=15)\n",
        "    plt.xticks(fontsize=15)\n",
        "    plt.savefig(path + prefix + '10_most_frequent_questions.png')\n",
        "    \n",
        "    plt.clf()\n",
        "    first_word = token_data[token_data['token_order'] == 0].token.value_counts()\n",
        "    first_word = first_word / first_word.sum()\n",
        "    ax = first_word.head(10).plot(kind=\"barh\", figsize=(24, 14), alpha=.7)\n",
        "    ax.invert_yaxis()\n",
        "    plt.savefig(path + prefix + 'distribution_of_10_most_frequent_question_starting_words.png')\n",
        "    \n",
        "    plt.clf()\n",
        "    word_token = token_data.query(\"pos != 'PUNCT'\").pos.value_counts()\n",
        "    word_token = word_token / word_token.sum()\n",
        "    ax = word_token.head(10).plot(kind=\"barh\", figsize=(24, 14), color='orange', alpha=.7)\n",
        "    ax.invert_yaxis()\n",
        "    plt.savefig(path + prefix + 'distribution_of_10_most_frequent_word_classes_in_questions.png')\n",
        "    \n",
        "    \n",
        "# Note: may need to edit the file path\n",
        "path = '/content/drive/MyDrive/11777/'\n",
        "analyze_questions(val_text, val_q_token_data, 'val_', path)\n",
        "analyze_questions(train_text, train_q_token_data, 'train_', path)\n",
        "analyze_questions(test_text, test_q_token_data, 'test_', path)"
      ],
      "metadata": {
        "id": "CG55ZU_45Xmk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For each dataset, analyze answers and create graphs and metrics files\n",
        "# Specifically, answer length, the most frequent answers, and the most frequent word classes in answers\n",
        "\n",
        "def analyze_answers(expanded_answers, text_df, token_data, prefix, path):\n",
        "    \n",
        "    avg_answer_len = token_data.groupby('doc_id').size().mean()\n",
        "    median_answer_len = token_data.groupby('doc_id').size().median()\n",
        "    std_dev_answer_len = token_data.groupby('doc_id').size().std()\n",
        "    with open(path + prefix + 'answer_metrics.txt', 'w') as metrics_file:\n",
        "        tsv_writer = csv.writer(metrics_file, delimiter='\\t')\n",
        "        tsv_writer.writerow(['avg answer length: ' + \"%.2f\" % avg_answer_len])\n",
        "        tsv_writer.writerow(['median answer length: ' + \"%.2f\" % median_answer_len])\n",
        "        tsv_writer.writerow(['std dev of answer length: ' + \"%.2f\" % std_dev_answer_len])\n",
        "\n",
        "    plt.clf()\n",
        "    counts = expanded_val_answers.value.value_counts()\n",
        "    counts = counts / 10\n",
        "    counts.head(10).plot(kind=\"barh\", figsize=(32, 14), color='green', alpha=.7)\n",
        "    plt.yticks(fontsize=10)\n",
        "    plt.xticks(fontsize=10)\n",
        "    plt.savefig(path + prefix + '10_most_frequent_answers.png')\n",
        "    \n",
        "    plt.clf()\n",
        "    word_token = token_data.query(\"pos != 'PUNCT'\").pos.value_counts()\n",
        "    word_token = word_token / word_token.sum()\n",
        "    ax = word_token.head(10).plot(kind=\"barh\", figsize=(24, 14), color='orange', alpha=.7)\n",
        "    ax.invert_yaxis()\n",
        "    plt.savefig(path + prefix + 'distribution_of_10_most_frequent_word_classes_in_answers.png')\n",
        "    \n",
        "    \n",
        "# Note: may need to edit the file path\n",
        "path = '/content/drive/MyDrive/11777/'\n",
        "analyze_answers(expanded_val_answers, val_text, val_a_token_data, 'val_', path)\n",
        "analyze_answers(expanded_train_answers, train_text, train_a_token_data, 'train_', path)"
      ],
      "metadata": {
        "id": "4H1HEBvVgjZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# spacy.explain('PART')"
      ],
      "metadata": {
        "id": "05JI9WgxxHYW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}