{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from evaluator import EvalAIAnswerProcessor, TextVQAAccuracyEvaluator\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval = TextVQAAccuracyEvaluator() # accuracy evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = pd.read_json(\"../Data/textvqa/TextVQA_0.5.1_val.json\")\n",
    "val_gt = val_data['data'].apply(pd.Series) # extract the data portion\n",
    "reference = val_gt[[\"question_id\", \"question\", \"answers\", \"question_tokens\"]] # extract question id, question text, answers, question_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model zoo\n",
    "models = {}\n",
    "models[\"multimodal-baselines\"] = [\"hotpot-lorra\", \"hotpot-without-mmt\", \"hotpot-without-object-label\", \"hotpot\"]\n",
    "models[\"unimodal-baselines\"] = [\"object\", \"ocr\", \"question\"]\n",
    "models[\"competitive-baselines\"] = [\"lorra\", \"m4c\", \"tap\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in prediction results from models\n",
    "pred = {}\n",
    "paths = {}\n",
    "for k,v in models.items():\n",
    "    pred[k] = {i:pd.read_json(\"./pred/\"+k+\"/textvqa-val-\"+i+\".json\") for i in v}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check to ensure question ids are the same\n",
    "# compute accuracy of each model on each question \n",
    "question_list = set(pred[\"multimodal-baselines\"][\"hotpot\"][\"question_id\"])\n",
    "for k,v in pred.items():\n",
    "    for i in v.keys():\n",
    "        # if the list of questions don't match\n",
    "        if set(pred[k][i][\"question_id\"]) != question_list: \n",
    "            print(\"issue\")\n",
    "        pred[k][i] = pred[k][i].merge(reference, on='question_id', how='inner')\n",
    "        pred[k][i].rename(columns={\"answer\":\"prediction\"}, inplace=True)\n",
    "        pred[k][i][\"accu_\"+i] = pred[k][i].apply(lambda row: eval.eval_pred_list([{\"pred_answer\":row.prediction, \"gt_answers\":row.answers}]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multimodal-baselines hotpot-lorra 0.291819999999999\n",
      "multimodal-baselines hotpot-without-mmt 0.4137999999999987\n",
      "multimodal-baselines hotpot-without-object-label 0.45415999999999984\n",
      "multimodal-baselines hotpot 0.45321999999999935\n",
      "unimodal-baselines object 0.1271999999999999\n",
      "unimodal-baselines ocr 0.25255999999999895\n",
      "unimodal-baselines question 0.18999999999999942\n",
      "competitive-baselines lorra 0.27573999999999876\n",
      "competitive-baselines m4c 0.39233999999999836\n",
      "competitive-baselines tap 0.49846\n"
     ]
    }
   ],
   "source": [
    "grade_book = reference.copy() # save the accuracy of all models on all questions\n",
    "for k,v in pred.items():\n",
    "    for i in v.keys():\n",
    "        print(k, i, np.mean(pred[k][i][\"accu_\"+i]))\n",
    "        grade_book = grade_book.merge(right=pred[k][i][[\"question_id\", \"accu_\"+i]], on=\"question_id\", how=\"inner\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"model_preds.pickle\", \"wb\") as f:\n",
    "    pickle.dump(pred, f)\n",
    "grade_book.to_csv(\"grade_book.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsets = [\"obj\", \"pos\"]\n",
    "for s in subsets:\n",
    "    f = open(\"./subsets/val_\"+s+\"_question.txt\", \"r\")\n",
    "    data = f.read().split(\",\")  \n",
    "    df = grade_book[grade_book[\"question_id\"].isin([int(d) for d in data])]\n",
    "    df.to_csv(\"./gradebooks/grade_book_\"+s+\".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
